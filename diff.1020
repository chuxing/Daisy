diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..499a618
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,2 @@
+*.o
+*.swp
diff --git a/cmd.txt b/cmd.txt
new file mode 100644
index 0000000..a444319
--- /dev/null
+++ b/cmd.txt
@@ -0,0 +1,7 @@
+make -j8
+watch "cat linux-3.18/1.log | grep 'Daisy'"
+qemu-system-x86_64 -hda ../linux-0.2.img -kernel arch/x86_64/boot/bzImage -append "root=/dev/sda console=ttyS0 rdinit=/bin/sh" -m 5120 -nographic &> 1.log
+pkill -f qemu
+
+pkill -f qemu; make -j8
+qemu-system-x86_64 -hda ../linux-0.2.img -kernel arch/x86_64/boot/bzImage -append "root=/dev/sda console=ttyS0 rdinit=/bin/sh" -m 5120 -nographic | grep 'Daisy'
diff --git a/go_pcm2.sh b/go_pcm2.sh
new file mode 100755
index 0000000..9bf65f3
--- /dev/null
+++ b/go_pcm2.sh
@@ -0,0 +1,2 @@
+qemu-system-x86_64 -hda ~/rootfs.img -kernel ~/git/Daisy/linux-3.18/arch/x86/boot/bzImage -append "root=/dev/sda rdinit=sbin/init noapic" -m 1024
+
diff --git a/linux-0.2.img b/linux-0.2.img
new file mode 100644
index 0000000..641fbce
Binary files /dev/null and b/linux-0.2.img differ
diff --git a/linux-3.18/arch/Kconfig b/linux-3.18/arch/Kconfig
index 05d7a8a..23a7535 100644
--- a/linux-3.18/arch/Kconfig
+++ b/linux-3.18/arch/Kconfig
@@ -530,4 +530,12 @@ config OLD_SIGACTION
 config COMPAT_OLD_SIGACTION
 	bool
 
+config  SCM
+	bool"hybrid with dram and scm"
+	default y
+
+config SCM_DEBUG
+	bool"scm_debug"
+	default y
+
 source "kernel/gcov/Kconfig"
diff --git a/linux-3.18/arch/x86/kernel/setup.c b/linux-3.18/arch/x86/kernel/setup.c
index ab08aa2..ac0fee8 100644
--- a/linux-3.18/arch/x86/kernel/setup.c
+++ b/linux-3.18/arch/x86/kernel/setup.c
@@ -856,6 +856,7 @@ dump_kernel_offset(struct notifier_block *self, unsigned long v, void *p)
 
 void __init setup_arch(char **cmdline_p)
 {
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	memblock_reserve(__pa_symbol(_text),
 			 (unsigned long)__bss_stop - (unsigned long)_text);
 
diff --git a/linux-3.18/arch/x86/mm/init.c b/linux-3.18/arch/x86/mm/init.c
index 66dba36..9043375 100644
--- a/linux-3.18/arch/x86/mm/init.c
+++ b/linux-3.18/arch/x86/mm/init.c
@@ -4,6 +4,7 @@
 #include <linux/swap.h>
 #include <linux/memblock.h>
 #include <linux/bootmem.h>	/* for max_low_pfn */
+#include <linux/scm.h>
 
 #include <asm/cacheflush.h>
 #include <asm/e820.h>
@@ -48,7 +49,7 @@ __ref void *alloc_low_pages(unsigned int num)
 {
 	unsigned long pfn;
 	int i;
-
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	if (after_bootmem) {
 		unsigned int order;
 
@@ -64,6 +65,7 @@ __ref void *alloc_low_pages(unsigned int num)
 		ret = memblock_find_in_range(min_pfn_mapped << PAGE_SHIFT,
 					max_pfn_mapped << PAGE_SHIFT,
 					PAGE_SIZE * num , PAGE_SIZE);
+		daisy_printk("ret: %lu\n", ret);
 		if (!ret)
 			panic("alloc_low_pages: can not alloc memory");
 		memblock_reserve(ret, PAGE_SIZE * num);
@@ -324,6 +326,7 @@ static void add_pfn_range_mapped(unsigned long start_pfn, unsigned long end_pfn)
 	if (start_pfn < (1UL<<(32-PAGE_SHIFT)))
 		max_low_pfn_mapped = max(max_low_pfn_mapped,
 					 min(end_pfn, 1UL<<(32-PAGE_SHIFT)));
+	daisy_printk("%s %s max_pfn_mapped %lu\n", __FILE__, __func__, max_pfn_mapped);
 }
 
 bool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn)
@@ -349,7 +352,7 @@ unsigned long __init_refok init_memory_mapping(unsigned long start,
 	struct map_range mr[NR_RANGE_MR];
 	unsigned long ret = 0;
 	int nr_range, i;
-
+	daisy_printk("%s %s start end %lu %lu\n", __FILE__, __func__, start, end);
 	pr_info("init_memory_mapping: [mem %#010lx-%#010lx]\n",
 	       start, end - 1);
 
@@ -385,7 +388,7 @@ static unsigned long __init init_range_memory_mapping(
 	unsigned long start_pfn, end_pfn;
 	unsigned long mapped_ram_size = 0;
 	int i;
-
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	for_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, NULL) {
 		u64 start = clamp_val(PFN_PHYS(start_pfn), r_start, r_end);
 		u64 end = clamp_val(PFN_PHYS(end_pfn), r_start, r_end);
@@ -443,7 +446,7 @@ static void __init memory_map_top_down(unsigned long map_start,
 	unsigned long addr;
 	unsigned long mapped_ram_size = 0;
 	unsigned long new_mapped_ram_size;
-
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	/* xen has big range in reserved near end of ram, skip it at first.*/
 	addr = memblock_find_in_range(map_start, map_end, PMD_SIZE, PMD_SIZE);
 	real_end = addr + PMD_SIZE;
@@ -476,9 +479,10 @@ static void __init memory_map_top_down(unsigned long map_start,
 			step_size = get_new_step_size(step_size);
 		mapped_ram_size += new_mapped_ram_size;
 	}
-
+	daisy_printk("out of while\n");
 	if (real_end < map_end)
 		init_range_memory_mapping(real_end, map_end);
+	daisy_printk("end of memory_map_top_down\n");
 }
 
 /**
@@ -530,6 +534,7 @@ void __init init_mem_mapping(void)
 {
 	unsigned long end;
 
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	probe_page_size_mask();
 
 #ifdef CONFIG_X86_64
<???TODO
@@ -547,6 +552,7 @@ void __init init_mem_mapping(void)
 	 */
 	if (memblock_bottom_up()) {
 		unsigned long kernel_end = __pa_symbol(_end);
+		daisy_printk("memblock_bottom_up\n");
 
 		/*
 		 * we need two separate calls here. This is because we want to
@@ -558,6 +564,7 @@ void __init init_mem_mapping(void)
 		memory_map_bottom_up(kernel_end, end);
 		memory_map_bottom_up(ISA_END_ADDRESS, kernel_end);
 	} else {
+		daisy_printk("NOT memblock_bottom_up\n");
 		memory_map_top_down(ISA_END_ADDRESS, end);
 	}
???TODO>
@@ -670,7 +677,10 @@ void __init free_initrd_mem(unsigned long start, unsigned long end)
 void __init zone_sizes_init(void)
 {
 	unsigned long max_zone_pfns[MAX_NR_ZONES];
+	int i;
 
+	daisy_printk("%s %s\n", __FILE__, __func__);
+	daisy_printk("%lu %lu %lu\n", max_low_pfn, max_pfn, SCM_PFN_NUM);
 	memset(max_zone_pfns, 0, sizeof(max_zone_pfns));
 
 #ifdef CONFIG_ZONE_DMA
@@ -679,10 +689,17 @@ void __init zone_sizes_init(void)
 #ifdef CONFIG_ZONE_DMA32
 	max_zone_pfns[ZONE_DMA32]	= MAX_DMA32_PFN;
 #endif
-	max_zone_pfns[ZONE_NORMAL]	= max_low_pfn;
+	max_zone_pfns[ZONE_NORMAL]	= max(max_low_pfn-SCM_PFN_NUM, MAX_DMA32_PFN);
 #ifdef CONFIG_HIGHMEM
 	max_zone_pfns[ZONE_HIGHMEM]	= max_pfn;
 #endif
+#ifdef CONFIG_SCM
+	max_zone_pfns[ZONE_SCM]	= max_low_pfn;
+#endif
+	/*print max_zone_pfns*/
+	for(i=0; i<MAX_NR_ZONES; ++i) {
+		daisy_printk("max_zone_pfns %d: %lu\n", i, max_zone_pfns[i]);
+	}
 
 	free_area_init_nodes(max_zone_pfns);
 }
diff --git a/linux-3.18/arch/x86/mm/init_64.c b/linux-3.18/arch/x86/mm/init_64.c
index 4e5dfec..033f1b9 100644
--- a/linux-3.18/arch/x86/mm/init_64.c
+++ b/linux-3.18/arch/x86/mm/init_64.c
@@ -615,7 +615,7 @@ kernel_physical_mapping_init(unsigned long start,
 	bool pgd_changed = false;
 	unsigned long next, last_map_addr = end;
 	unsigned long addr;
-
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	start = (unsigned long)__va(start);
 	end = (unsigned long)__va(end);
 	addr = start;
@@ -660,6 +660,8 @@ void __init initmem_init(void)
 
 void __init paging_init(void)
 {
+	daisy_printk("%s %s\n", __FILE__, __func__);
+
 	sparse_memory_present_with_active_regions(MAX_NUMNODES);
 	sparse_init();
 
@@ -1057,6 +1059,7 @@ static void __init register_page_bootmem_info(void)
 
 void __init mem_init(void)
 {
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	pci_iommu_alloc();
 
 	/* clear_bss() already clear the empty_zero_page */
diff --git a/linux-3.18/arch/x86/mm/numa.c b/linux-3.18/arch/x86/mm/numa.c
index 1a88370..5353509 100644
--- a/linux-3.18/arch/x86/mm/numa.c
+++ b/linux-3.18/arch/x86/mm/numa.c
@@ -681,6 +681,7 @@ static int __init dummy_numa_init(void)
  */
 void __init x86_numa_init(void)
 {
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	if (!numa_off) {
 #ifdef CONFIG_ACPI_NUMA
 		if (!numa_init(x86_acpi_numa_init))
diff --git a/linux-3.18/arch/x86/syscalls/syscall_64.tbl b/linux-3.18/arch/x86/syscalls/syscall_64.tbl
index 281150b..d6ae2e2 100644
--- a/linux-3.18/arch/x86/syscalls/syscall_64.tbl
+++ b/linux-3.18/arch/x86/syscalls/syscall_64.tbl
@@ -328,6 +328,10 @@
 319	common	memfd_create		sys_memfd_create
 320	common	kexec_file_load		sys_kexec_file_load
 321	common	bpf			sys_bpf
+322 64 		p_mmap 		sys_p_mmap
+323 64 		p_search_big_region_node		sys_p_search_big_region_node
+324	64		p_alloc_and_insert				sys_p_alloc_and_insert
+
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/linux-3.18/include/linux/gfp.h b/linux-3.18/include/linux/gfp.h
index 41b30fd..88153cf 100644
--- a/linux-3.18/include/linux/gfp.h
+++ b/linux-3.18/include/linux/gfp.h
@@ -14,7 +14,9 @@ struct vm_area_struct;
 #define ___GFP_HIGHMEM		0x02u
 #define ___GFP_DMA32		0x04u
 #define ___GFP_MOVABLE		0x08u
-#define ___GFP_WAIT		0x10u
+/* __GFP_WAIT change to 0x2000000u */
+#define ___GFP_SCM		0x10u
+
 #define ___GFP_HIGH		0x20u
 #define ___GFP_IO		0x40u
 #define ___GFP_FS		0x80u
@@ -34,6 +36,8 @@ struct vm_area_struct;
 #define ___GFP_NO_KSWAPD	0x400000u
 #define ___GFP_OTHER_NODE	0x800000u
 #define ___GFP_WRITE		0x1000000u
+
+#define ___GFP_WAIT		0x2000000u
 /* If the above are modified, __GFP_BITS_SHIFT may need updating */
 
 /*
@@ -48,8 +52,9 @@ struct vm_area_struct;
 #define __GFP_DMA	((__force gfp_t)___GFP_DMA)
 #define __GFP_HIGHMEM	((__force gfp_t)___GFP_HIGHMEM)
 #define __GFP_DMA32	((__force gfp_t)___GFP_DMA32)
+#define __GFP_SCM	((__force gfp_t)___GFP_SCM)
 #define __GFP_MOVABLE	((__force gfp_t)___GFP_MOVABLE)  /* Page is movable */
-#define GFP_ZONEMASK	(__GFP_DMA|__GFP_HIGHMEM|__GFP_DMA32|__GFP_MOVABLE)
+#define GFP_ZONEMASK	(__GFP_DMA|__GFP_HIGHMEM|__GFP_DMA32|__GFP_MOVABLE|__GFP_SCM)
 /*
  * Action modifiers - doesn't change the zoning
  *
@@ -97,7 +102,7 @@ struct vm_area_struct;
  */
 #define __GFP_NOTRACK_FALSE_POSITIVE (__GFP_NOTRACK)
 
-#define __GFP_BITS_SHIFT 25	/* Room for N __GFP_FOO bits */
+#define __GFP_BITS_SHIFT 26	/* Room for N __GFP_FOO bits */
 #define __GFP_BITS_MASK ((__force gfp_t)((1 << __GFP_BITS_SHIFT) - 1))
 
 /* This equals 0, but use constants in case they ever change */
@@ -155,6 +160,8 @@ struct vm_area_struct;
 /* 4GB DMA on some platforms */
 #define GFP_DMA32	__GFP_DMA32
 
+#define GFP_SCM	__GFP_SCM
+
 /* Convert GFP flags to their corresponding migrate type */
 static inline int gfpflags_to_migratetype(const gfp_t gfp_flags)
 {
@@ -168,6 +175,12 @@ static inline int gfpflags_to_migratetype(const gfp_t gfp_flags)
 		((gfp_flags & __GFP_RECLAIMABLE) != 0);
 }
 
+#ifdef CONFIG_SCM
+#define OPT_ZONE_SCM ZONE_SCM
+#else
+#define OPT_ZONE_SCM ZONE_NORMAL
+#endif
+
 #ifdef CONFIG_HIGHMEM
 #define OPT_ZONE_HIGHMEM ZONE_HIGHMEM
 #else
@@ -219,19 +232,21 @@ static inline int gfpflags_to_migratetype(const gfp_t gfp_flags)
  * ZONES_SHIFT must be <= 2 on 32 bit platforms.
  */
 
-#if 16 * ZONES_SHIFT > BITS_PER_LONG
-#error ZONES_SHIFT too large to create GFP_ZONE_TABLE integer
-#endif
+//#if 16 * ZONES_SHIFT > BITS_PER_LONG
+//#error ZONES_SHIFT too large to create GFP_ZONE_TABLE integer
+//#endif
 
+/* enum to unsigned long to avoid "left shift count >= width of type" */
 #define GFP_ZONE_TABLE ( \
-	(ZONE_NORMAL << 0 * ZONES_SHIFT)				      \
-	| (OPT_ZONE_DMA << ___GFP_DMA * ZONES_SHIFT)			      \
-	| (OPT_ZONE_HIGHMEM << ___GFP_HIGHMEM * ZONES_SHIFT)		      \
-	| (OPT_ZONE_DMA32 << ___GFP_DMA32 * ZONES_SHIFT)		      \
-	| (ZONE_NORMAL << ___GFP_MOVABLE * ZONES_SHIFT)			      \
-	| (OPT_ZONE_DMA << (___GFP_MOVABLE | ___GFP_DMA) * ZONES_SHIFT)	      \
-	| (ZONE_MOVABLE << (___GFP_MOVABLE | ___GFP_HIGHMEM) * ZONES_SHIFT)   \
-	| (OPT_ZONE_DMA32 << (___GFP_MOVABLE | ___GFP_DMA32) * ZONES_SHIFT)   \
+	((unsigned long)ZONE_NORMAL << 0 * ZONES_SHIFT)				      \
+	| ((unsigned long)OPT_ZONE_DMA << ___GFP_DMA * ZONES_SHIFT)			      \
+	| ((unsigned long)OPT_ZONE_HIGHMEM << ___GFP_HIGHMEM * ZONES_SHIFT)		      \
+	| ((unsigned long)OPT_ZONE_DMA32 << ___GFP_DMA32 * ZONES_SHIFT)		      \
+	| ((unsigned long)ZONE_NORMAL << ___GFP_MOVABLE * ZONES_SHIFT)			      \
+	| ((unsigned long)OPT_ZONE_DMA << (___GFP_MOVABLE | ___GFP_DMA) * ZONES_SHIFT)	      \
+	| ((unsigned long)ZONE_MOVABLE << (___GFP_MOVABLE | ___GFP_HIGHMEM) * ZONES_SHIFT)   \
+	| ((unsigned long)OPT_ZONE_DMA32 << (___GFP_MOVABLE | ___GFP_DMA32) * ZONES_SHIFT)   \
+    | ((unsigned long)OPT_ZONE_SCM << ___GFP_SCM * ZONES_SHIFT)  \
 )
 
 /*
diff --git a/linux-3.18/include/linux/mm.h b/linux-3.18/include/linux/mm.h
index b464611..7be8345 100644
--- a/linux-3.18/include/linux/mm.h
+++ b/linux-3.18/include/linux/mm.h
@@ -140,6 +140,7 @@ extern unsigned int kobjsize(const void *objp);
 #define VM_HUGEPAGE	0x20000000	/* MADV_HUGEPAGE marked this vma */
 #define VM_NOHUGEPAGE	0x40000000	/* MADV_NOHUGEPAGE marked this vma */
 #define VM_MERGEABLE	0x80000000	/* KSM may merge identical pages */
+#define VM_PCM		0x0000000100000000
 
 #if defined(CONFIG_X86)
 # define VM_PAT		VM_ARCH_1	/* PAT reserves whole VMA at once (x86) */
diff --git a/linux-3.18/include/linux/mm_types.h b/linux-3.18/include/linux/mm_types.h
index 6e0b286..f7c6c54 100644
--- a/linux-3.18/include/linux/mm_types.h
+++ b/linux-3.18/include/linux/mm_types.h
<???TODO
@@ -308,6 +308,8 @@ struct vm_area_struct {
 #ifdef CONFIG_NUMA
 	struct mempolicy *vm_policy;	/* NUMA policy for the VMA */
 #endif
+
+	unsigned long scm_id;
 };
 ???TODO>
 struct core_thread {
diff --git a/linux-3.18/include/linux/mmzone.h b/linux-3.18/include/linux/mmzone.h
index ffe66e3..815ef11 100644
--- a/linux-3.18/include/linux/mmzone.h
+++ b/linux-3.18/include/linux/mmzone.h
@@ -318,6 +318,9 @@ enum zone_type {
 	 */
 	ZONE_HIGHMEM,
 #endif
+#ifdef CONFIG_SCM
+	ZONE_SCM,
+#endif
 	ZONE_MOVABLE,
 	__MAX_NR_ZONES
 };
@@ -791,6 +794,8 @@ static inline bool pgdat_is_empty(pg_data_t *pgdat)
 
 extern struct mutex zonelists_mutex;
 void build_all_zonelists(pg_data_t *pgdat, struct zone *zone);
+void print_all_pgdat(void);
+
 void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx);
 bool zone_watermark_ok(struct zone *z, unsigned int order,
 		unsigned long mark, int classzone_idx, int alloc_flags);
diff --git a/linux-3.18/include/linux/page-flags-layout.h b/linux-3.18/include/linux/page-flags-layout.h
index da52366..77b078c 100644
--- a/linux-3.18/include/linux/page-flags-layout.h
+++ b/linux-3.18/include/linux/page-flags-layout.h
@@ -17,6 +17,8 @@
 #define ZONES_SHIFT 1
 #elif MAX_NR_ZONES <= 4
 #define ZONES_SHIFT 2
+#elif MAX_NR_ZONES <= 8
+#define ZONES_SHIFT 3
 #else
 #error ZONES_SHIFT -- too many zones configured adjust calculation
 #endif
diff --git a/linux-3.18/include/linux/printk.h b/linux-3.18/include/linux/printk.h
index d78125f..0ef4f7a 100644
--- a/linux-3.18/include/linux/printk.h
+++ b/linux-3.18/include/linux/printk.h
@@ -141,6 +141,11 @@ int printk_emit(int facility, int level,
 asmlinkage __printf(1, 2) __cold
 int printk(const char *fmt, ...);
 
+#ifdef CONFIG_SCM
+asmlinkage __printf(1, 2) __cold
+int daisy_printk(const char *fmt, ...);
+#endif
+
 /*
  * Special printk facility for scheduler/timekeeping use only, _DO_NOT_USE_ !
  */
diff --git a/linux-3.18/include/linux/scm.h b/linux-3.18/include/linux/scm.h
new file mode 100644
index 0000000..b74c4b9
--- /dev/null
+++ b/linux-3.18/include/linux/scm.h
@@ -0,0 +1,93 @@
+/* TODO mutex & lock */
+
+#ifndef _SCM_H
+#define _SCM_H
+
+#include <linux/rbtree.h>
+#include <linux/list.h>
+
+/**
+ * magic number
+ * ptable addr
+ * heaptable addr
+ * data
+*/
+
+/* 1024M=1G pfns 1024*1024/4*/
+#define SCM_PFN_NUM 262144UL
+#define SCM_PTABLE_PFN_NUM 1024
+#define SCM_MAGIC 0x01234567
+
+/* flags in struct node */
+/* TODO use bit such as 0001 0002 0004 0008 0010 */
+#define BIG_MEM_REGION 0
+#define SMALL_MEM_REGION 1
+#define HEAP_REGION 2
+
+struct ptable_node;
+struct hptable_node;
+
+struct scm_head {
+	unsigned long magic;
+	struct rb_root ptable_rb;
+	struct rb_root hptable_rb;
+	unsigned long total_size; /* the total memory length (bytes) */
+	unsigned long len; /* item number */
+	char data[0];
+};
+
+/**
+ * SCM persist table node
+ *
+ * FLAGS:
+ * 0. big memory region
+ * 1. small memory region
+ * 2. heap region
+ *
+ * Requirement:
+ * sizeof(hptable_node) === sizeof(ptable_node)
+ *
+ * Note:
+ * _id of 1 & 2 must unique!
+ * */
+struct ptable_node {
+	u64 _id;
+	union {
+		u64 phys_addr;
+		u64 offset;
+	};
+	u64 size;
+	unsigned long flags; /* BIG_MEM_REGION or SMALL_MEM_REGION */
+	u64 hptable_id; /* 0 or real _id */
+	struct rb_node	ptable_rb;
+};
+
+struct hptable_node {
+	u64 _id;
+	u64 phys_addr;
+	u64 size;
+	unsigned long flags; /* HEAP_REGION */
+	u64 dummy;
+	struct rb_node	hptable_rb;
+};
+
+struct table_freelist {
+	void *node_addr;
+	struct list_head list;
+};
+
+/* linux/mm/scm.c */
+void scm_ptable_boot(void);
+void scm_freelist_boot(void);
+
+struct ptable_node *search_big_region_node(u64 _id);
+struct ptable_node *search_small_region_node(u64 _id);
+struct hptable_node *search_heap_region_node(u64 _id);
+int insert_big_region_node(u64 _id, u64 phys_addr, u64 size);
+int insert_small_region_node(u64 _id, u64 offset, u64 size, u64 hptable_id);
+int insert_heap_region_node(u64 _id, u64 phys_addr, u64 size);
+int delete_big_region_node(u64 _id);
+int delete_small_region_node(u64 _id);
+int delete_heap_region_node(u64 _id);
+
+#endif /* _SCM_H */
diff --git a/linux-3.18/include/uapi/asm-generic/unistd.h b/linux-3.18/include/uapi/asm-generic/unistd.h
index 22749c1..54567a0 100644
--- a/linux-3.18/include/uapi/asm-generic/unistd.h
+++ b/linux-3.18/include/uapi/asm-generic/unistd.h
<???TODO
@@ -708,8 +708,19 @@ __SYSCALL(__NR_memfd_create, sys_memfd_create)
 #define __NR_bpf 280
 __SYSCALL(__NR_bpf, sys_bpf)
 
+/*PCM system calls are defined here*/
+#define __NR_p_mmap 322
+__SYSCALL(__NR_p_mmap, sys_p_mmap)
+#define __NR_p_search_big_region_node 323
+__SYSCALL(__NR_p_search_big_region_node, sys_p_search_big_region_node)
+#define __NR_p_alloc_and_insert 324
+__SYSCALL(__NR_p_alloc_and_insert, sys_p_alloc_and_insert)
+
+/*PCM system call definitions end here*/
+
 #undef __NR_syscalls
-#define __NR_syscalls 281
+#define __NR_syscalls 284
+
???TODO>
 /*
  * All syscalls below here should go away really,
diff --git a/linux-3.18/init/main.c b/linux-3.18/init/main.c
index 321d0ce..25bde23 100644
--- a/linux-3.18/init/main.c
+++ b/linux-3.18/init/main.c
@@ -78,6 +78,7 @@
 #include <linux/context_tracking.h>
 #include <linux/random.h>
 #include <linux/list.h>
+#include <linux/scm.h>
 
 #include <asm/io.h>
 #include <asm/bugs.h>
@@ -490,7 +491,9 @@ static void __init mm_init(void)
 	 * bigger than MAX_ORDER unless SPARSEMEM.
 	 */
 	page_cgroup_init_flatmem();
+	/* In mem_init, memblock die */
 	mem_init();
+	/* In mem_init, memblock die */
 	kmem_cache_init();
 	percpu_init_late();
 	pgtable_init();
@@ -529,6 +532,9 @@ asmlinkage __visible void __init start_kernel(void)
 	page_address_init();
 	pr_notice("%s", linux_banner);
 	setup_arch(&command_line);
+
+	scm_ptable_boot();
+
 	mm_init_cpumask(&init_mm);
 	setup_command_line(command_line);
 	setup_nr_cpu_ids();
@@ -559,8 +565,13 @@ asmlinkage __visible void __init start_kernel(void)
 	vfs_caches_init_early();
 	sort_main_extable();
 	trap_init();
+
 	mm_init();
 
+	/* After mm_init we can use kmalloc and we can never use memblock*/
+	print_all_pgdat();
+	scm_freelist_boot();
+
 	/*
 	 * Set up the scheduler prior starting any interrupts (such as the
 	 * timer interrupt). Full topology setup happens at smp_init()
diff --git a/linux-3.18/kernel/printk/printk.c b/linux-3.18/kernel/printk/printk.c
index ced2b84..854101a 100644
--- a/linux-3.18/kernel/printk/printk.c
+++ b/linux-3.18/kernel/printk/printk.c
@@ -1849,6 +1849,22 @@ asmlinkage __visible int printk(const char *fmt, ...)
 }
 EXPORT_SYMBOL(printk);
 
+#ifdef CONFIG_SCM
+asmlinkage __visible int daisy_printk(const char *fmt, ...) {
+	va_list args;
+	int r;
+	char buf[128] = "[Daisy] ";
+	va_start(args, fmt);
+	vsnprintf(buf+8, sizeof(buf)-8, fmt, args);
+#ifdef CONFIG_SCM_DEBUG
+	r = printk(buf);
+#endif
+	va_end(args);
+	return r;
+}
+EXPORT_SYMBOL(daisy_printk);
+#endif
+
 #else /* CONFIG_PRINTK */
 
 #define LOG_LINE_MAX		0
diff --git a/linux-3.18/mm/Makefile b/linux-3.18/mm/Makefile
index 8405eb0..ec572cf 100644
--- a/linux-3.18/mm/Makefile
+++ b/linux-3.18/mm/Makefile
@@ -18,7 +18,7 @@ obj-y			:= filemap.o mempool.o oom_kill.o \
 			   mm_init.o mmu_context.o percpu.o slab_common.o \
 			   compaction.o vmacache.o \
 			   interval_tree.o list_lru.o workingset.o \
-			   iov_iter.o debug.o $(mmu-y)
+			   iov_iter.o debug.o scm.o $(mmu-y)
 
 obj-y += init-mm.o
 
diff --git a/linux-3.18/mm/memory.c b/linux-3.18/mm/memory.c
index d5f2ae9..e9b3c8f 100644
--- a/linux-3.18/mm/memory.c
+++ b/linux-3.18/mm/memory.c
@@ -61,6 +61,7 @@
 #include <linux/string.h>
 #include <linux/dma-debug.h>
 #include <linux/debugfs.h>
+#include <linux/scm.h>
 
 #include <asm/io.h>
 #include <asm/pgalloc.h>
 <???TODO
@@ -2877,7 +2878,7 @@ static void do_fault_around(struct vm_area_struct *vma, unsigned long address,
 	vmf.flags = flags;
 	vma->vm_ops->map_pages(vma, &vmf);
 }
-
+struct page* pg=NULL;
 static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 		unsigned long address, pmd_t *pmd,
 		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
@@ -2901,7 +2902,40 @@ static int do_read_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 		pte_unmap_unlock(pte, ptl);
 	}
 
-	ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+	if(vma->vm_flags & VM_PCM)
+	{
+		daisy_printk("===== scm_id in vma = %d\n", vma->scm_id);
+		struct ptable_node *p_node = search_big_region_node(vma->scm_id);
+		if (!p_node) p_node = search_small_region_node(vma->scm_id);
+
+		if (!p_node) {
+			daisy_printk("===== fatal error: p_node is null");
+			return -1;
+		}
+
+		fault_page = (pfn_to_page(p_node->phys_addr >> PAGE_SHIFT));
+		daisy_printk("allocated page: pfn %p\n", PFN_PHYS(page_to_pfn(fault_page)));
+
+		/*
+		if(pg)
+		{
+			fault_page=pg;
+			printk("============count=%lx,mapcount=%lx\n",pg->_count,pg->_mapcount);
+		}
+		else
+		{
+			pg=alloc_page(__GFP_MOVABLE);
+			printk("============alloc pfn=%lx\n",page_to_pfn(pg));
+			printk("============count=%lx,mapcount=%lx\n",pg->_count,pg->_mapcount);
+			fault_page=pg;//32M
+		}
+		*/
+		ret=0;
+	}
+	else
+	{
+		ret = __do_fault(vma, address, pgoff, flags, &fault_page);
+	}
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 		return ret;
 ???TODO>
 
@@ -3362,7 +3396,8 @@ int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	 */
 	if (flags & FAULT_FLAG_USER)
 		mem_cgroup_oom_enable();
-
+	if(vma->vm_flags & VM_PCM)
+		printk("PCM!!!!!!!\n");
 	ret = __handle_mm_fault(mm, vma, address, flags);
 
 	if (flags & FAULT_FLAG_USER) {
diff --git a/linux-3.18/mm/mmap.c b/linux-3.18/mm/mmap.c
index ae91989..cdf1a55 100644
--- a/linux-3.18/mm/mmap.c
+++ b/linux-3.18/mm/mmap.c
@@ -3320,6 +3320,286 @@ static int reserve_mem_notifier(struct notifier_block *nb,
 	return NOTIFY_OK;
 }
 
+unsigned long mmap_region2(struct file *file, unsigned long addr,
+		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff, unsigned long scm_id)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma, *prev;
+	int error;
+	struct rb_node **rb_link, *rb_parent;
+	unsigned long charged = 0;
+
+	/* Check against address space limit. */
+	if (!may_expand_vm(mm, len >> PAGE_SHIFT)) {
+		unsigned long nr_pages;
+
+		/*
+		 * MAP_FIXED may remove pages of mappings that intersects with
+		 * requested mapping. Account for the pages it would unmap.
+		 */
+		if (!(vm_flags & MAP_FIXED))
+			return -ENOMEM;
+
+		nr_pages = count_vma_pages_range(mm, addr, addr + len);
+
+		if (!may_expand_vm(mm, (len >> PAGE_SHIFT) - nr_pages))
+			return -ENOMEM;
+	}
+
+	/* Clear old maps */
+	error = -ENOMEM;
+munmap_back:
+	if (find_vma_links(mm, addr, addr + len, &prev, &rb_link, &rb_parent)) {
+		if (do_munmap(mm, addr, len))
+			return -ENOMEM;
+		goto munmap_back;
+	}
+
+	/*
+	 * Private writable mapping: check memory availability
+	 */
+	if (accountable_mapping(file, vm_flags)) {
+		charged = len >> PAGE_SHIFT;
+		if (security_vm_enough_memory_mm(mm, charged))
+			return -ENOMEM;
+		vm_flags |= VM_ACCOUNT;
+	}
+
+	/*
+	 * Can we just expand an old mapping?
+	 */
+	if (!(vm_flags & VM_PCM)) {
+		vma = vma_merge(mm, prev, addr, addr + len, vm_flags, NULL, file, pgoff, NULL);
+		if (vma)
+			goto out;
+	}
+
+	/*
+	 * Determine the object being mapped and call the appropriate
+	 * specific mapper. the address has already been validated, but
+	 * not unmapped, but the maps are removed from the list.
+	 */
+	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+	if (!vma) {
+		error = -ENOMEM;
+		goto unacct_error;
+	}
+
+	vma->vm_mm = mm;
+	vma->vm_start = addr;
+	vma->vm_end = addr + len;
+	vma->vm_flags = vm_flags;
+	vma->vm_page_prot = vm_get_page_prot(vm_flags);
+	vma->vm_pgoff = pgoff;
+	vma->scm_id = scm_id;
+	INIT_LIST_HEAD(&vma->anon_vma_chain);
+
+	if (file) {
+		if (vm_flags & VM_DENYWRITE) {
+			error = deny_write_access(file);
+			if (error)
+				goto free_vma;
+		}
+		if (vm_flags & VM_SHARED) {
+			error = mapping_map_writable(file->f_mapping);
+			if (error)
+				goto allow_write_and_free_vma;
+		}
+
+		/* ->mmap() can change vma->vm_file, but must guarantee that
+		 * vma_link() below can deny write-access if VM_DENYWRITE is set
+		 * and map writably if VM_SHARED is set. This usually means the
+		 * new file must not have been exposed to user-space, yet.
+		 */
+		vma->vm_file = get_file(file);
+		error = file->f_op->mmap(file, vma);
+		if (error)
+			goto unmap_and_free_vma;
+
+		/* Can addr have changed??
+		 *
+		 * Answer: Yes, several device drivers can do it in their
+		 *         f_op->mmap method. -DaveM
+		 * Bug: If addr is changed, prev, rb_link, rb_parent should
+		 *      be updated for vma_link()
+		 */
+		WARN_ON_ONCE(addr != vma->vm_start);
+
+		addr = vma->vm_start;
+		vm_flags = vma->vm_flags;
+	} else if (vm_flags & VM_SHARED) {
+		error = shmem_zero_setup(vma);
+		if (error)
+			goto free_vma;
+	}
+
+	vma_link(mm, vma, prev, rb_link, rb_parent);
+	/* Once vma denies write, undo our temporary denial count */
+	if (file) {
+		if (vm_flags & VM_SHARED)
+			mapping_unmap_writable(file->f_mapping);
+		if (vm_flags & VM_DENYWRITE)
+			allow_write_access(file);
+	}
+	file = vma->vm_file;
+out:
+	perf_event_mmap(vma);
+
+	vm_stat_account(mm, vm_flags, file, len >> PAGE_SHIFT);
+	if (vm_flags & VM_LOCKED) {
+		if (!((vm_flags & VM_SPECIAL) || is_vm_hugetlb_page(vma) ||
+					vma == get_gate_vma(current->mm)))
+			mm->locked_vm += (len >> PAGE_SHIFT);
+		else
+			vma->vm_flags &= ~VM_LOCKED;
+	}
+
+	if (file)
+		uprobe_mmap(vma);
+
+	/*
+	 * New (or expanded) vma always get soft dirty status.
+	 * Otherwise user-space soft-dirty page tracker won't
+	 * be able to distinguish situation when vma area unmapped,
+	 * then new mapped in-place (which must be aimed as
+	 * a completely new data area).
+	 */
+	vma->vm_flags |= VM_SOFTDIRTY;
+
+	vma_set_page_prot(vma);
+
+	return addr;
+
+unmap_and_free_vma:
+	vma->vm_file = NULL;
+	fput(file);
+
+	/* Undo any partial mapping done by a device driver. */
+	unmap_region(mm, vma, prev, vma->vm_start, vma->vm_end);
+	charged = 0;
+	if (vm_flags & VM_SHARED)
+		mapping_unmap_writable(file->f_mapping);
+allow_write_and_free_vma:
+	if (vm_flags & VM_DENYWRITE)
+		allow_write_access(file);
+free_vma:
+	kmem_cache_free(vm_area_cachep, vma);
+unacct_error:
+	if (charged)
+		vm_unacct_memory(charged);
+	return error;
+}
+
+unsigned long do_p_mmap_pgoff(unsigned long addr,
+			unsigned long len, unsigned long prot,
+			unsigned long flags, unsigned long pgoff,unsigned long* populate, unsigned long scm_id)
+{
+	struct mm_struct *mm = current->mm;
+	vm_flags_t vm_flags;
+
+	/*
+	 * Does the application expect PROT_READ to imply PROT_EXEC?
+	 *
+	 * (the exception is when the underlying filesystem is noexec
+	 *  mounted, in which case we dont add PROT_EXEC.)
+	 */
+	if ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))
+			prot |= PROT_EXEC;
+
+	if (!len)
+		return -EINVAL;
+
+	if (!(flags & MAP_FIXED))
+		addr = round_hint_to_min(addr);
+
+	/* Careful about overflows.. */
+	len = PAGE_ALIGN(len);
+	if (!len)
+		return -ENOMEM;
+
+	/* offset overflow? */
+	if ((pgoff + (len >> PAGE_SHIFT)) < pgoff)
+		return -EOVERFLOW;
+
+	/* Too many mappings? */
+	if (mm->map_count > sysctl_max_map_count)
+		return -ENOMEM;
+
+	/* Obtain the address to map to. we verify (or select) it and ensure
+	 * that it represents a valid section of the address space.
+	 */
+	addr = get_unmapped_area(NULL, addr, len, pgoff, flags);
+	if (addr & ~PAGE_MASK)
+		return addr;
+
+	/* Do simple checking here so the lower-level routines won't have
+	 * to. we assume access permissions have been handled by the open
+	 * of the memory object, so we don't do any here.
+	 */
+	vm_flags = calc_vm_prot_bits(prot) | calc_vm_flag_bits(flags) |
+			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC | VM_PCM | VM_LOCKED | VM_SHARED;
+
+	if (flags & MAP_LOCKED)
+		if (!can_do_mlock())
+			return -EPERM;
+
+	if (mlock_future_check(mm, vm_flags, len))
+		return -EAGAIN;
+
+
+	switch (flags & MAP_TYPE) {
+	case MAP_SHARED:
+		if (vm_flags & (VM_GROWSDOWN | VM_GROWSUP))
+			return -EINVAL;
+		/*
+		 * Ignore pgoff.
+		 */
+		pgoff = 0;
+		vm_flags |= VM_SHARED | VM_MAYSHARE;
+		break;
+	case MAP_PRIVATE:
+		/*
+		 * Set pgoff according to addr for anon_vma.
+		 */
+		pgoff = addr >> PAGE_SHIFT;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	/*
+	 * Set 'VM_NORESERVE' if we should not account for the
+	 * memory use of this mapping.
+	 */
+	if (flags & MAP_NORESERVE) {
+		/* We honor MAP_NORESERVE if allowed to overcommit */
+		if (sysctl_overcommit_memory != OVERCOMMIT_NEVER)
+			vm_flags |= VM_NORESERVE;
+	}
+
+	addr = mmap_region2(NULL, addr, len, vm_flags, pgoff, scm_id);
+	*populate=len;
+	return addr;
+}
+
+
+SYSCALL_DEFINE4(p_mmap, unsigned long, addr, unsigned long, len,
+		unsigned long, prot, unsigned long, id)
+{
+	unsigned int flags = MAP_ANONYMOUS | MAP_SHARED;
+	unsigned long retval = -EBADF;
+	struct mm_struct *mm = current->mm;
+	unsigned long populate;
+	printk("p_map : 0x%lx , %ld , 0x%lx ,%ld\n",addr,len,prot,id);
+
+	down_write(&mm->mmap_sem);
+	retval = do_p_mmap_pgoff(addr, len, prot, flags, 0, &populate, id);
+	up_write(&mm->mmap_sem);
+	mm_populate(retval, populate);
+
+	return retval;
+}
+
 static struct notifier_block reserve_mem_nb = {
 	.notifier_call = reserve_mem_notifier,
 };
diff --git a/linux-3.18/mm/mmzone.c b/linux-3.18/mm/mmzone.c
index bf34fb8..21dbd9a 100644
--- a/linux-3.18/mm/mmzone.c
+++ b/linux-3.18/mm/mmzone.c
@@ -51,6 +51,15 @@ static inline int zref_in_nodemask(struct zoneref *zref, nodemask_t *nodes)
 #endif /* CONFIG_NUMA */
 }
 
+/* special design for scm: if scm just ignore zonelist mechanism */
+static int zone_ok_or_scm(struct zoneref *z, enum zone_type highest_zoneidx) {
+	if (highest_zoneidx != ZONE_SCM) {
+		return zonelist_zone_idx(z) > highest_zoneidx;
+	} else {
+		return zonelist_zone_idx(z) != ZONE_SCM;
+	}
+}
+
 /* Returns the next zone at or below highest_zoneidx in a zonelist */
 struct zoneref *next_zones_zonelist(struct zoneref *z,
 					enum zone_type highest_zoneidx,
@@ -61,11 +70,12 @@ struct zoneref *next_zones_zonelist(struct zoneref *z,
 	 * Find the next suitable zone to use for the allocation.
 	 * Only filter based on nodemask if it's set
 	 */
+	/* do a trick for SCM zone */
 	if (likely(nodes == NULL))
-		while (zonelist_zone_idx(z) > highest_zoneidx)
+		while (zone_ok_or_scm(z, highest_zoneidx))
 			z++;
 	else
-		while (zonelist_zone_idx(z) > highest_zoneidx ||
+		while (zone_ok_or_scm(z, highest_zoneidx) ||
 				(z->zone && !zref_in_nodemask(z, nodes)))
 			z++;
 
diff --git a/linux-3.18/mm/nobootmem.c b/linux-3.18/mm/nobootmem.c
index 90b5046..06b007a 100644
--- a/linux-3.18/mm/nobootmem.c
+++ b/linux-3.18/mm/nobootmem.c
@@ -174,12 +174,13 @@ void __init reset_all_zones_managed_pages(void)
 unsigned long __init free_all_bootmem(void)
 {
 	unsigned long pages;
+	daisy_printk("%s %s\n", __FILE__, __func__);
 
 	reset_all_zones_managed_pages();
 
 	/*
 	 * We need to use NUMA_NO_NODE instead of NODE_DATA(0)->node_id
-	 *  because in some case like Node0 doesn't have RAM installed
+	 *  because in some case like Node0 doesn't have RAM pinstalled
 	 *  low ram will be on Node1
 	 */
 	pages = free_low_memory_core_early();
diff --git a/linux-3.18/mm/page_alloc.c b/linux-3.18/mm/page_alloc.c
index 616a2c9..7a72221 100644
--- a/linux-3.18/mm/page_alloc.c
+++ b/linux-3.18/mm/page_alloc.c
@@ -185,6 +185,9 @@ int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES-1] = {
 #ifdef CONFIG_HIGHMEM
 	 32,
 #endif
+#ifdef CONFIG_SCM
+	 32,
+#endif
 	 32,
 };
 
@@ -201,6 +204,9 @@ static char * const zone_names[MAX_NR_ZONES] = {
 #ifdef CONFIG_HIGHMEM
 	 "HighMem",
 #endif
+#ifdef CONFIG_SCM
+	 "Scm",
+#endif
 	 "Movable",
 };
 
@@ -3354,19 +3360,27 @@ static void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)
 static int build_zonelists_node(pg_data_t *pgdat, struct zonelist *zonelist,
 				int nr_zones)
 {
-	struct zone *zone;
+	struct zone *zone, *scm_zone = NULL;
 	enum zone_type zone_type = MAX_NR_ZONES;
 
+	daisy_printk("build_zonelists_node pgdat->node_id: %d, nr_zones: %d\n", pgdat->node_id, pgdat->nr_zones);
+	/*do a trick here, to let SCM zone to be special*/
 	do {
 		zone_type--;
 		zone = pgdat->node_zones + zone_type;
-		if (populated_zone(zone)) {
+		daisy_printk("zone->present_pages: %s %lu\n", zone->name, zone->present_pages);
+		if (zone_type == ZONE_SCM) {
+			scm_zone = zone;
+		} else if (populated_zone(zone) && zone_type != ZONE_SCM) {
 			zoneref_set_zone(zone,
 				&zonelist->_zonerefs[nr_zones++]);
 			check_highest_zone(zone_type);
 		}
 	} while (zone_type);
 
+	if (scm_zone) {
+		zoneref_set_zone(scm_zone, &zonelist->_zonerefs[nr_zones++]);
+	}
 	return nr_zones;
 }
 
@@ -3557,9 +3571,11 @@ static void build_zonelists_in_node_order(pg_data_t *pgdat, int node)
 	int j;
 	struct zonelist *zonelist;
 
+	daisy_printk("build_zonelists_in_node_order\n");
 	zonelist = &pgdat->node_zonelists[0];
 	for (j = 0; zonelist->_zonerefs[j].zone != NULL; j++)
 		;
+	daisy_printk("build_zonelists_in_node_order j: %d\n", j);
 	j = build_zonelists_node(NODE_DATA(node), zonelist, j);
 	zonelist->_zonerefs[j].zone = NULL;
 	zonelist->_zonerefs[j].zone_idx = 0;
@@ -3638,10 +3654,45 @@ static int default_zonelist_order(void)
 
 static void set_zonelist_order(void)
 {
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	if (user_zonelist_order == ZONELIST_ORDER_DEFAULT)
 		current_zonelist_order = default_zonelist_order();
 	else
 		current_zonelist_order = user_zonelist_order;
+	daisy_printk("current_zonelist_order: %d\n", current_zonelist_order);
+}
+
+static void print_pgdat(pg_data_t *pgdat)
+{
+	struct zone *z;
+	int i, j;
+
+	daisy_printk("%s %s\n", __FILE__, __func__);
+	/*print node_zones*/
+	for (i=0; i<MAX_NR_ZONES; ++i) {
+		daisy_printk("%s ", pgdat->node_zones[i].name);
+	}
+	daisy_printk("...\n");
+	/*print node_zonelists*/
+	for (i=0; i<MAX_ZONELISTS; ++i) {
+		for (j=0; j<(MAX_ZONES_PER_ZONELIST+1); ++j) {
+			z = pgdat->node_zonelists[i]._zonerefs[j].zone;
+			if (z == NULL) {
+				break;
+			}
+			daisy_printk("zone->managed_pages: %s %lu\n", z->name, z->managed_pages);
+		}
+		daisy_printk("...\n");
+	}
+}
+
+void print_all_pgdat(void)
+{
+	int nid;
+	for_each_online_node(nid) {
+		pg_data_t *pgdat = NODE_DATA(nid);
+		print_pgdat(pgdat);
+	}
 }
 
 static void build_zonelists(pg_data_t *pgdat)
@@ -3653,6 +3704,7 @@ static void build_zonelists(pg_data_t *pgdat)
 	struct zonelist *zonelist;
 	int order = current_zonelist_order;
 
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	/* initialize zonelists */
 	for (i = 0; i < MAX_ZONELISTS; i++) {
 		zonelist = pgdat->node_zonelists + i;
@@ -3670,6 +3722,7 @@ static void build_zonelists(pg_data_t *pgdat)
 	j = 0;
 
 	while ((node = find_next_best_node(local_node, &used_mask)) >= 0) {
+		daisy_printk("find_next_best_node: %d\n", node);
 		/*
 		 * We don't want to pressure a particular node.
 		 * So adding penalty to the first node in same
@@ -3693,6 +3746,7 @@ static void build_zonelists(pg_data_t *pgdat)
 	}
 
 	build_thisnode_zonelists(pgdat);
+	print_pgdat(pgdat);
 }
 
 /* Construct the zonelist performance cache - see further mmzone.h */
@@ -3702,6 +3756,7 @@ static void build_zonelist_cache(pg_data_t *pgdat)
 	struct zonelist_cache *zlc;
 	struct zoneref *z;
 
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	zonelist = &pgdat->node_zonelists[0];
 	zonelist->zlcache_ptr = zlc = &zonelist->zlcache;
 	bitmap_zero(zlc->fullzones, MAX_ZONES_PER_ZONELIST);
@@ -3821,6 +3876,7 @@ static int __build_all_zonelists(void *data)
 	for_each_online_node(nid) {
 		pg_data_t *pgdat = NODE_DATA(nid);
 
+		daisy_printk("nid: %d\n", nid);
 		build_zonelists(pgdat);
 		build_zonelist_cache(pgdat);
 	}
@@ -3864,13 +3920,16 @@ static int __build_all_zonelists(void *data)
  */
 void __ref build_all_zonelists(pg_data_t *pgdat, struct zone *zone)
 {
+	daisy_printk("%s %s\n", __FILE__, __func__);
 	set_zonelist_order();
 
 	if (system_state == SYSTEM_BOOTING) {
+		daisy_printk("%s: Booting\n", __func__);
 		__build_all_zonelists(NULL);
 		mminit_verify_zonelist();
 		cpuset_init_current_mems_allowed();
 	} else {
+		daisy_printk("%s: Not booting\n", __func__);
 #ifdef CONFIG_MEMORY_HOTPLUG
 		if (zone)
 			setup_zone_pageset(zone);
@@ -3893,7 +3952,7 @@ void __ref build_all_zonelists(pg_data_t *pgdat, struct zone *zone)
 	else
 		page_group_by_mobility_disabled = 0;
 
-	printk("Built %i zonelists in %s order, mobility grouping %s.  "
+	daisy_printk("Built %i zonelists in %s order, mobility grouping %s.  "
 		"Total pages: %ld\n",
 			nr_online_nodes,
 			zonelist_order_name[current_zonelist_order],
@@ -5311,6 +5370,8 @@ void __init free_area_init_nodes(unsigned long *max_zone_pfn)
 	unsigned long start_pfn, end_pfn;
 	int i, nid;
 
+	daisy_printk("%s %s\n", __FILE__, __func__);
+
 	/* Record where the zone boundaries are */
 	memset(arch_zone_lowest_possible_pfn, 0,
 				sizeof(arch_zone_lowest_possible_pfn));
diff --git a/linux-3.18/mm/scm.c b/linux-3.18/mm/scm.c
new file mode 100644
index 0000000..59b5f44
--- /dev/null
+++ b/linux-3.18/mm/scm.c
@@ -0,0 +1,454 @@
+#include <linux/scm.h>
+#include <linux/memblock.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/rbtree.h>
+#include <linux/syscalls.h>
+
+static struct scm_head *scm_head;
+static struct table_freelist *table_freelist;
+
+/* FOR DEBUG */
+static u64 freecount = 0; // count freelist
+static void scm_print_freelist(void)
+{
+	struct table_freelist *tmp;
+	int i=0;
+	daisy_printk("freelist %lu: ", freecount);
+	list_for_each_entry(tmp, &table_freelist->list, list) {
+		daisy_printk("%lu %lu\t", tmp->node_addr, ((unsigned long)tmp->node_addr-(unsigned long)&scm_head->data)/sizeof(struct ptable_node));
+		++i;
+		if (i>=10) break;
+	}
+	daisy_printk("\n");
+}
+
+static void scm_print_pnode(struct ptable_node *n)
+{
+	if (n) {
+		daisy_printk("id: %lu, vaddr %lu\n", n->_id, n);
+	} else {
+		daisy_printk("NULL\n");
+	}
+}
+
+static void scm_fake_initdata(void)
+{
+	struct ptable_node *pnode;
+	struct hptable_node *hnode;
+
+	if (!scm_head) return;
+
+	pnode = (struct ptable_node *)((char *)&scm_head->data + 3*sizeof(struct ptable_node));
+	scm_head->ptable_rb.rb_node = &pnode->ptable_rb;
+	hnode = (struct hptable_node *)((char *)&scm_head->data + 5*sizeof(struct ptable_node));
+	scm_head->hptable_rb.rb_node = &hnode->hptable_rb;
+}
+
+void scm_full_test(void)
+{
+	struct ptable_node *n;
+	struct page *page;
+
+	insert_big_region_node(345, 0, 0);
+	scm_print_freelist();
+	insert_small_region_node(344, 0, 0, 557);
+	insert_heap_region_node(557, 0, 0);
+	insert_big_region_node(342, 0, 0);
+	scm_print_freelist();
+	n = search_big_region_node(342);
+	scm_print_pnode(n);
+	n = search_small_region_node(344);
+	scm_print_pnode(n);
+	n = search_heap_region_node(557);
+	scm_print_pnode(n);
+	delete_big_region_node(342);
+	delete_heap_region_node(557);
+	delete_small_region_node(344);
+	delete_big_region_node(345);
+	scm_print_freelist();
+	page = alloc_pages(GFP_KERNEL | GFP_SCM, 0);
+	daisy_printk("alloc_pages: %s %lu %lu %lu\n", page_zone(page)->name, page_to_pfn(page), PFN_PHYS(page_to_pfn(page)), page_address(page));
+	page = alloc_pages(GFP_KERNEL | GFP_DMA, 0);
+	daisy_printk("alloc_pages: %s %lu %lu %lu\n", page_zone(page)->name, page_to_pfn(page), PFN_PHYS(page_to_pfn(page)), page_address(page));
+	page = alloc_pages(GFP_KERNEL, 0);
+	daisy_printk("alloc_pages: %s %lu %lu %lu\n", page_zone(page)->name, page_to_pfn(page), PFN_PHYS(page_to_pfn(page)), page_address(page));
+}
+/* end FOR DEBUG */
+
+static void reserve_scm_ptable_memory(void)
+{
+	unsigned long size;
+	phys_addr_t phys;
+
+	/* get the first 1024 scm pages */
+	size = SCM_PTABLE_PFN_NUM * PAGE_SIZE;
+	/* pages in ZONE_SCM */
+	phys = PFN_PHYS(max_pfn_mapped)-(SCM_PFN_NUM<<PAGE_SHIFT);
+	memblock_reserve(phys, size);
+	scm_head = (struct scm_head*)__va(phys);
+
+	daisy_printk("scm_start_phys: %lu scm_head vaddr %lu\n", phys, scm_head);
+	daisy_printk("Get start pfn: %luï¼Œ max_pfn: %lu\n", phys >> PAGE_SHIFT, max_pfn_mapped);
+	/* record the size */
+	/* TODO if scm has old data, total_size cannot change, do a realloc; now just check */
+	if (scm_head->magic == SCM_MAGIC && scm_head->total_size !=size) {
+		daisy_printk("TODO we need a warning or realloc here.\n");
+	}
+	scm_head->total_size = size;
+}
+
+/* Init a total new SCM (no data) */
+static void scm_ptable_init(void)
+{
+	scm_head->magic = SCM_MAGIC;
+	scm_head->ptable_rb = RB_ROOT;
+	scm_head->hptable_rb = RB_ROOT;
+	/* calc scm_head->len */
+	scm_head->len = (scm_head->total_size - sizeof(struct scm_head))/sizeof(struct ptable_node);
+	/* do i need a whole memset (set to 0)? */
+}
+
+static void scm_reserve_used_memory(void) {
+	struct rb_node *nd;
+	/* ptable */
+	if (!RB_EMPTY_ROOT(&scm_head->ptable_rb)) {
+		for (nd = rb_first(&scm_head->ptable_rb); nd; nd = rb_next(nd)) {
+			struct ptable_node *touch;
+			touch = rb_entry(nd, struct ptable_node, ptable_rb);
+			/* ignore small memory region */
+			if (touch->flags == BIG_MEM_REGION) {
+				memblock_reserve(touch->phys_addr, touch->size);
+			}
+		}
+	}
+	/* hptable */
+	if (!RB_EMPTY_ROOT(&scm_head->hptable_rb)) {
+		for (nd = rb_first(&scm_head->hptable_rb); nd; nd = rb_next(nd)) {
+			struct hptable_node *touch;
+			touch = rb_entry(nd, struct hptable_node, hptable_rb);
+			memblock_reserve(touch->phys_addr, touch->size);
+		}
+	}
+}
+
+/**
+ * scm persist table boot step
+ * reference to: numa_alloc_distance & numa_reset_distance
+ */
+void scm_ptable_boot(void)
+{
+	reserve_scm_ptable_memory();
+
+	/**
+	 * check magic number to decide how to init
+	 * 1. clear scm, just set scm_head data
+	 * 2. traverse tree to create freelist (do it later)
+	 * */
+	daisy_printk("scm_head: %lu %lu %lu %lu\n",
+			scm_head->magic,
+			scm_head->ptable_rb,
+			scm_head->hptable_rb,
+			scm_head->len);
+	if (scm_head->magic != SCM_MAGIC) {
+		/* this is a new SCM */
+		scm_ptable_init();
+		//scm_fake_initdata();
+	} else {
+		/* SCM with data! */
+		scm_reserve_used_memory();
+	}
+}
+
+/**
+ * Just traverse the tree to init the freelist in DRAM
+ * memblock reserve at the same time
+ */
+void scm_freelist_boot(void)
+{
+	struct table_freelist *tmp;
+	unsigned long index;
+	struct rb_node *nd;
+	char *usage_map = (char *)kmalloc(scm_head->len, GFP_KERNEL);
+
+	table_freelist = (struct table_freelist *) kmalloc(sizeof(struct table_freelist), GFP_KERNEL);
+	INIT_LIST_HEAD(&table_freelist->list);
+
+	for (index = 0; index < scm_head->len; ++index) {
+		usage_map[index] = 0;
+	}
+	/* ptable */
+	if (!RB_EMPTY_ROOT(&scm_head->ptable_rb)) {
+		for (nd = rb_first(&scm_head->ptable_rb); nd; nd = rb_next(nd)) {
+			struct ptable_node *touch;
+			touch = rb_entry(nd, struct ptable_node, ptable_rb);
+			/* ignore small memory region */
+			if (touch->flags == BIG_MEM_REGION) {
+				index = ((unsigned long) touch - (unsigned long) &scm_head->data) / sizeof(struct ptable_node);
+				usage_map[index] = 1;
+			}
+		}
+	}
+	/* hptable */
+	if (!RB_EMPTY_ROOT(&scm_head->hptable_rb)) {
+		for (nd = rb_first(&scm_head->hptable_rb); nd; nd = rb_next(nd)) {
+			struct hptable_node *touch;
+			touch = rb_entry(nd, struct hptable_node, hptable_rb);
+			index = ((unsigned long) touch - (unsigned long) &scm_head->data) / sizeof(struct hptable_node);
+			usage_map[index] = 1;
+		}
+	}
+	/* freelist */
+	for (index = 0; index < scm_head->len; ++index) {
+		if (usage_map[index] == 0) {
+			tmp = (struct table_freelist *) kmalloc(sizeof(struct table_freelist), GFP_KERNEL);
+			tmp->node_addr = (char *) &scm_head->data + index * sizeof(struct ptable_node);
+			list_add_tail(&tmp->list, &table_freelist->list);
+			freecount++;
+		}
+	}
+	/* TODO test SCM is not new */
+	scm_print_freelist();
+	scm_full_test();
+	kfree(usage_map);
+}
+
+/* pop an item from freelist, free the item, return the addr (NULL if no more) */
+static void *get_freenode_addr(void)
+{
+	void *ret;
+	struct table_freelist *entry;
+	if (list_empty(&table_freelist->list)) {
+		return NULL;
+	}
+	entry = list_first_entry(&table_freelist->list, struct table_freelist, list);
+	ret = entry->node_addr;
+	list_del(&entry->list);
+	freecount--;
+	kfree(entry);
+	return ret;
+}
+
+/* return -1 if error & 0 if success */
+static int insert_ptable_node_rb(u64 _id, u64 phys_addr, u64 size, u64 hptable_id, unsigned long flags)
+{
+	struct rb_node **n = &scm_head->ptable_rb.rb_node;
+	struct rb_node *parent = NULL;
+	struct ptable_node *new, *touch;
+	new = (struct ptable_node *)get_freenode_addr();
+	if (!new) {
+		return -1;
+	}
+	new->_id = _id;
+	new->phys_addr = phys_addr;
+	new->size = size;
+	new->flags = flags;
+	new->hptable_id = hptable_id;
+
+	/* insert to rbtree */
+	while (*n) {
+		parent = *n;
+		touch = rb_entry(parent, struct ptable_node, ptable_rb);
+		if (_id < touch->_id) {
+			n = &(*n)->rb_left;
+		} else if (_id > touch->_id) {
+			n = &(*n)->rb_right;
+		} else {
+			return -1;
+		}
+	}
+	rb_link_node(&new->ptable_rb, parent, n);
+	rb_insert_color(&new->ptable_rb, &scm_head->ptable_rb);
+	return 0;
+}
+
+static int insert_hptable_node_rb(u64 _id, u64 phys_addr, u64 size)
+{
+	struct rb_node **n = &scm_head->hptable_rb.rb_node;
+	struct rb_node *parent = NULL;
+	struct hptable_node *new, *touch;
+	new = (struct hptable_node *)get_freenode_addr();
+	if (!new) {
+		return -1;
+	}
+	new->_id = _id;
+	new->phys_addr = phys_addr;
+	new->size = size;
+	new->flags = HEAP_REGION;
+	/* insert to rbtree */
+	while (*n) {
+		parent = *n;
+		touch = rb_entry(parent, struct hptable_node, hptable_rb);
+		if (_id < touch->_id) {
+			n = &(*n)->rb_left;
+		} else if (_id > touch->_id) {
+			n = &(*n)->rb_right;
+		} else {
+			return -1;
+		}
+	}
+	rb_link_node(&new->hptable_rb, parent, n);
+	rb_insert_color(&new->hptable_rb, &scm_head->hptable_rb);
+	return 0;
+}
+
+int insert_big_region_node(u64 _id, u64 phys_addr, u64 size)
+{
+	return insert_ptable_node_rb(_id, phys_addr, size, 0, BIG_MEM_REGION);
+}
+
+int insert_small_region_node(u64 _id, u64 offset, u64 size, u64 hptable_id)
+{
+	return insert_ptable_node_rb(_id, offset, size, hptable_id, SMALL_MEM_REGION);
+}
+
+int insert_heap_region_node(u64 _id, u64 phys_addr, u64 size)
+{
+	return insert_hptable_node_rb(_id, phys_addr, size);
+}
+
+/* return NULL if not found */
+static struct ptable_node *search_ptable_node_rb(u64 _id, unsigned long flags)
+{
+	struct rb_node *n;
+	struct ptable_node *touch;
+	if (flags != BIG_MEM_REGION && flags != SMALL_MEM_REGION) {
+		return NULL;
+	}
+	n = scm_head->ptable_rb.rb_node;
+	while (n) {
+		touch = rb_entry(n, struct ptable_node, ptable_rb);
+		if (_id < touch->_id) {
+			n = n->rb_left;
+		} else if (_id > touch->_id) {
+			n = n->rb_right;
+		} else {
+			/* do a simple check */
+			if (touch->flags == flags) {
+				return touch;
+			} else {
+				return NULL;
+			}
+		}
+	}
+	return NULL;
+}
+
+/* return NULL if not found */
+static struct hptable_node *search_hptable_node_rb(u64 _id)
+{
+	struct rb_node *n;
+	struct hptable_node *touch;
+	n = scm_head->hptable_rb.rb_node;
+	while (n) {
+		touch = rb_entry(n, struct hptable_node, hptable_rb);
+		if (_id < touch->_id) {
+			n = n->rb_left;
+		} else if (_id > touch->_id) {
+			n = n->rb_right;
+		} else {
+			/* do a simple check */
+			if (touch->flags == HEAP_REGION) {
+				return touch;
+			} else {
+				return NULL;
+			}
+		}
+	}
+	return NULL;
+}
+
+struct ptable_node *search_big_region_node(u64 _id)
+{
+	return search_ptable_node_rb(_id, BIG_MEM_REGION);
+}
+
+struct ptable_node *search_small_region_node(u64 _id)
+{
+	return search_ptable_node_rb(_id, SMALL_MEM_REGION);
+}
+
+struct hptable_node *search_heap_region_node(u64 _id)
+{
+	return search_hptable_node_rb(_id);
+}
+
+static void add_freenode_addr(void *addr)
+{
+	struct table_freelist *tmp;
+	tmp= (struct table_freelist *)kmalloc(sizeof(struct table_freelist), GFP_KERNEL);
+	tmp->node_addr = addr;
+	list_add(&tmp->list, &table_freelist->list);
+	freecount++;
+}
+
+/* -1 error, 0 success */
+static int delete_ptable_node_rb(u64 _id, unsigned long flags)
+{
+	struct ptable_node *n;
+	n = search_ptable_node_rb(_id, flags);
+	if (!n) {
+		return -1;
+	}
+	rb_erase(&n->ptable_rb, &scm_head->ptable_rb);
+	add_freenode_addr((void *)n);
+	return 0;
+}
+
+static int delete_hptable_node_rb(u64 _id)
+{
+	struct hptable_node *n;
+	n = search_hptable_node_rb(_id);
+	if (!n) {
+		return -1;
+	}
+	rb_erase(&n->hptable_rb, &scm_head->hptable_rb);
+	add_freenode_addr((void *)n);
+	return 0;
+}
+
+int delete_big_region_node(u64 _id)
+{
+	return delete_ptable_node_rb(_id, BIG_MEM_REGION);
+}
+
+int delete_small_region_node(u64 _id)
+{
+	return delete_ptable_node_rb(_id, SMALL_MEM_REGION);
+}
+
+int delete_heap_region_node(u64 _id)
+{
+	return delete_hptable_node_rb(_id);
+}
+
+SYSCALL_DEFINE1(p_search_big_region_node, unsigned long, id) {
+	struct ptable_node *node = search_big_region_node(id);
+	return (node != NULL);
+}
+
+SYSCALL_DEFINE2(p_alloc_and_insert, unsigned long, id, int, size) {
+	int iRet = 0;
+	struct page *page;
+
+	page = alloc_pages(GFP_KERNEL | GFP_SCM, 0);
+	if (page == NULL) {
+		daisy_printk("error: alloc_pages\n");
+		return -1;
+	}
+
+	void *pAddr = (page_to_pfn(page) << PAGE_SHIFT);
+	if (pAddr == NULL) {
+		daisy_printk("error: page_address");
+		return -1;
+	} else {
+		daisy_printk("page's phys addr = %p\n", pAddr);
+	}
+
+	iRet = insert_big_region_node(id, (u64)pAddr, size);
+	if (iRet != 0) {
+		daisy_printk("error: insert_big_region_node\n");
+	}
+
+	return iRet;
+}
diff --git a/linux-3.18/mm/swap.c b/linux-3.18/mm/swap.c
index 8a12b33..930f658 100644
--- a/linux-3.18/mm/swap.c
+++ b/linux-3.18/mm/swap.c
@@ -887,6 +887,7 @@ void lru_add_drain_all(void)
 	mutex_unlock(&lock);
 }
 
+extern struct page* pg;
 /**
  * release_pages - batched page_cache_release()
  * @pages: array of pages to release
@@ -907,7 +908,10 @@ void release_pages(struct page **pages, int nr, bool cold)
 
 	for (i = 0; i < nr; i++) {
 		struct page *page = pages[i];
-
+		if(page_zone(page)->name[0] == 'S' && page_zone(page)->name[1] == 'c' && page_zone(page)->name[2] == 'm') {
+			daisy_printk("find scm page, continue\n");
+			continue;
+		}
 		if (unlikely(PageCompound(page))) {
 			if (zone) {
 				spin_unlock_irqrestore(&zone->lru_lock, flags);
diff --git a/pcmapi/Makefile b/pcmapi/Makefile
new file mode 100644
index 0000000..3aba581
--- /dev/null
+++ b/pcmapi/Makefile
@@ -0,0 +1,10 @@
+CC=gcc
+CFLAGS=-I.
+
+all: test.o p_mmap.o
+	$(CC) -static -o test test.o p_mmap.o -I.
+
+clean:
+	@rm -rf *.o test
+
+
diff --git a/pcmapi/p_mmap.c b/pcmapi/p_mmap.c
new file mode 100644
index 0000000..db56f46
--- /dev/null
+++ b/pcmapi/p_mmap.c
@@ -0,0 +1,207 @@
+#include "p_mmap.h"
+
+static char *pAddr = NULL;
+static const int iBitsCount = (SHM_SIZE) / 9 * 8;
+
+static void* p_mmap(void* addr,unsigned long len,unsigned long prot,unsigned long id) {
+	return (void*)syscall(__NR_p_mmap, addr, len, prot, id);
+}
+
+static int p_search_big_region_node(unsigned long id) {
+	return (int)syscall(__NR_p_search_big_region_node, id);
+}
+
+static int p_alloc_and_insert(unsigned long id, int size) {
+    return (int)syscall(__NR_p_alloc_and_insert, id, size);
+}
+
+int p_init() {
+    key_t key;
+    int shmid;
+    int mode;
+    int iRet = 0;
+
+    /* make the key: */
+    if ((key = ftok("/bin", 'R')) == -1) {
+        perror("ftok");
+        exit(1);
+    }
+
+    /* connect to (and possibly create) the segment: */
+    if ((shmid = shmget(key, SHM_SIZE, 0777 | IPC_CREAT)) == -1) {
+        perror("shmget");
+        exit(1);
+    }
+
+    /* attach to the segment to get a pointer to it: */
+    pAddr = shmat(shmid, (void *)0, 0);
+    if (pAddr == (char *)(-1)) {
+        perror("shmat");
+        exit(1);
+    }
+
+    printf("get and attach succee! addr=%p\n", pAddr);
+
+    return 0;
+}
+
+int p_clear() {
+    if (pAddr == NULL) {
+        printf("error: call p_init first\n");
+        return -1;
+    }
+
+    memset(pAddr, 0, SHM_SIZE/9);
+
+    return 0;
+}
+
+void* p_malloc(int size) {
+    if (size < 0) {
+        printf("error: p_malloc, size must be greater than 0");
+        return NULL;
+    }
+
+    if (pAddr == NULL) {
+        printf("error: call p_init first\n");
+        return NULL;
+    }
+
+    char curChar;
+    unsigned char mask;
+    
+    enum {
+        STOP = 0,
+        LOOKING
+    } state;
+
+    state = STOP;
+    int iStartBit = 0;
+
+    int n;
+    for (n=0; n<iBitsCount; n++) {
+        mask = 1 << (7 - n%8);
+        if (!(pAddr[n/8] & mask)) {
+            // nth bit is empty
+
+            switch (state) {
+                case STOP:
+                    iStartBit = n;
+                    state = LOOKING;
+                case LOOKING:
+                    if (n - iStartBit + 1>= size) {
+                        // we find it 
+                        printf("we find it, ready to set bit\n"); 
+                        set_bit_to_one(iStartBit, n);
+                        return pAddr + SHM_SIZE/9 + iStartBit; 
+                    }
+
+                    break;
+                default:
+                    break;
+            }
+        } else {
+            // nth bit is not empty
+            switch (state) {
+                case LOOKING:
+                    state = STOP;
+                    break;
+                default:
+                    break;
+            }
+        }
+    }
+
+    return NULL;
+}
+
+void set_bit_to_one(int iStartBit, int iEnd) {
+    unsigned char mask;
+    //printf("in set_bit_to_one, %d, %d", iStartBit, iEnd);
+    int n;
+    for (n=iStartBit; n<=iEnd; n++) {
+        mask = 1 << (7 - n%8);
+        pAddr[n/8] |= mask;
+
+        //printf("mask=%d,after set: %d\n", mask,pAddr[n/8]&mask);
+    }
+}
+
+int p_free(void *addr, int size) {
+    if (!addr || size <= 0) {
+        printf("invalid arguments\n");
+        return -1;
+    }
+
+    if (addr < pAddr + SHM_SIZE/9 || addr + size > pAddr + SHM_SIZE - 1) {
+        printf("addr out of range\n"); 
+        return -1;
+    }
+    
+    int nth = (char*)addr - pAddr - SHM_SIZE/9;
+    unsigned char mask;
+    int n;
+    for (n=nth ; n<nth+size; n++) {
+        mask = 1 << (7 - n%8);
+        pAddr[n/8] &= ~mask;
+    }
+
+    return 0;
+}
+
+void *p_new(int pId, int iSize) {
+    /*
+    if (iSize < 4096) {
+        return NULL;
+    }
+    */
+
+    int iRet = 0;
+
+    iRet = p_search_big_region_node(pId);
+    printf("return from p_search_big_region_node: %d\n", iRet);
+    if (iRet) {
+        printf("id %d already exist\n", pId);
+        //return NULL;
+    }
+
+    iRet = p_alloc_and_insert(pId, iSize);
+    printf("return from p_alloc_and_insert: %d\n", (int)iRet);
+    if (iRet != 0) {
+        printf("error: p_alloc_and_insert\n");
+        //return NULL;
+    }
+
+    void *pAddr = p_mmap(NULL, iSize, PROT_READ | PROT_WRITE, pId);
+    if (!pAddr) {
+        printf("p_mmap return NULL\n");
+    }
+
+    return pAddr;
+}
+
+int p_delete(int pId) {
+    /*
+    p_unmap(pId);
+    p_tab_delete(pId);
+    */
+   
+    return 0;
+}
+
+void *p_get(int pId, int iSize) {
+    int iRet = 0;
+    
+    iRet = p_search_big_region_node(pId);
+    if (!iRet) {
+        printf("cannot find %d in big region\n", pId);
+        return NULL;
+    }
+
+    void *pAddr = p_mmap(NULL, iSize, PROT_READ | PROT_WRITE, pId);
+    if (!pAddr) {
+        printf("p_mmap return NULL\n");
+    }
+
+    return pAddr;
+}
diff --git a/pcmapi/p_mmap.h b/pcmapi/p_mmap.h
new file mode 100644
index 0000000..5d2ab71
--- /dev/null
+++ b/pcmapi/p_mmap.h
@@ -0,0 +1,56 @@
+#ifndef _P_MMAP_H
+#define _P_MMAP_H
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/types.h>
+#include <sys/ipc.h>
+#include <sys/shm.h>
+#include <sys/syscall.h>
+#include <sys/mman.h>
+#include <linux/unistd.h>
+
+#define __NR_p_mmap 322
+#define __NR_p_search_big_region_node 323
+#define __NR_p_alloc_and_insert 324
+
+#define SHM_SIZE    (1024*9)
+
+struct tagMemoryBlock {
+    void    *pStart;
+    size_t  iLen;
+    struct  tagMemoryBlock *next;
+};
+
+typedef struct tagMemoryBlock memoryBlock;
+
+struct tagBuddy {
+    memoryBlock *pstMemBlock;
+};
+
+typedef struct tagBuddy Buddy;
+
+int p_init();
+
+/*
+* æ¸…é™¤pcmä¸Šæ–¹çš„å…ƒæ•°æ®
+*/
+int p_clear();
+
+void *p_malloc(int size);
+
+int p_free(void *addr, int size);
+
+void *p_new(int pId, int size);
+
+int p_delete(int pId);
+
+void *p_get(int pId, int iSize);
+
+/*** helper functions ***/
+
+void set_bit_to_one(int iStartBit, int iEnd);
+
+#endif
diff --git a/pcmapi/test.c b/pcmapi/test.c
new file mode 100644
index 0000000..83d6078
--- /dev/null
+++ b/pcmapi/test.c
@@ -0,0 +1,66 @@
+#include "test.h"
+#include "p_mmap.h"
+
+int main(int argc, char **argv) {
+    int iRet = 0;
+    char *ptr = NULL;
+
+    /*
+    iRet = p_init();
+    if (iRet < 0) {
+        printf("error: p_init\n");
+        return -1;
+    }
+
+    if (argc > 1) {
+        iRet = p_clear();
+        if (iRet < 0) {
+            printf("error: p_clear\n");
+            return -1;
+        }
+    }
+    
+    ptr = (char *)p_malloc(1);
+    printf("return from p_malloc 1, addr=%p\n", ptr);
+    */
+    
+    /*
+    p_free(ptr, 1);
+    printf("after free %p\n", ptr);
+
+    ptr = (char *)p_malloc(1);
+    printf("return from p_malloc 1, addr=%p\n", ptr);
+
+    ptr = (char *)p_malloc(1);
+    printf("return from p_malloc 1, addr=%p\n", ptr);
+
+    ptr = (char *)p_malloc(10);
+    printf("return from p_malloc 10, addr=%p\n", ptr);
+
+    ptr = (char *)p_malloc(100);
+    printf("return from p_malloc 100, addr=%p\n", ptr);
+
+    p_free(ptr, 100);
+    printf("after free %p\n", ptr);
+
+    ptr = (char *)p_malloc(100);
+    printf("return from p_malloc 100, addr=%p\n", ptr);
+    */
+    
+    printf("ready to call p_get\n");
+    ptr = p_get(23, 4096);
+    if (!ptr) {
+        printf("ready to call p_new\n");
+        ptr = p_new(23, 4096);
+        if (!ptr) {
+            printf("p_new failed\n");
+            return -1;
+        }
+    }
+
+    
+    int *tmp = (int *)ptr;
+    printf("=== ptr = %p, value = %d, ready to add 1\n", ptr,*tmp);
+    *tmp += 1;
+    return 0;
+}
diff --git a/pcmapi/test.h b/pcmapi/test.h
new file mode 100644
index 0000000..b8182e5
--- /dev/null
+++ b/pcmapi/test.h
@@ -0,0 +1,5 @@
+#ifndef _TEST_H
+#define _TEST_H
+
+
+#endif
diff --git a/pcmtest.c b/pcmtest.c
new file mode 100644
index 0000000..3d60c26
--- /dev/null
+++ b/pcmtest.c
@@ -0,0 +1,34 @@
+/*
+ ============================================================================
+ Name        : pcmtest.c
+ Author      : Menooker
+ Version     :
+ Copyright   : Your copyright notice
+ Description : Hello World in C, Ansi-style
+ ============================================================================
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <sys/syscall.h>
+#include <linux/unistd.h>
+#include <unistd.h>
+#include <sys/mman.h>
+#include <sys/types.h>
+#define __NR_p_mmap 322
+/*
+ SYSCALL_DEFINE4(p_mmap, unsigned long, addr, unsigned long, len,
+		unsigned long, prot,	unsigned long, id)
+ */
+
+void* p_mmap(void* addr,unsigned long len,unsigned long prot,unsigned long id)
+{
+	return (void*)syscall(__NR_p_mmap, addr, len,prot,id);
+}
+
+int main(void) {
+	long * addr=(long*)p_mmap(NULL,4096,PROT_READ | PROT_WRITE, 2);
+	printf("Call!%lx %lx\n",addr,*addr);
+	*addr=0xcafebabe;
+	return EXIT_SUCCESS;
+}
diff --git a/rootfs.img b/rootfs.img
new file mode 100644
index 0000000..81997b9
Binary files /dev/null and b/rootfs.img differ
